{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNAAn3pHjOPQI3u5WSfp6w3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#ğŸ“˜ Lab Assignment 2\n","Text Representation using BoW, TF-IDF & Word2Vec"],"metadata":{"id":"2QlRtTvljk1g"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"Gci7Vsb7ijMR","executionInfo":{"status":"ok","timestamp":1770089561956,"user_tz":-330,"elapsed":21,"user":{"displayName":"OMKAR SHINDE","userId":"17705111733778157300"}}},"outputs":[],"source":["documents = [\n","    \"I love natural language processing\",\n","    \"Natural language processing is fun\",\n","    \"I love machine learning\",\n","    \"Machine learning and NLP are closely related\"\n","]\n"]},{"cell_type":"markdown","source":["#ğŸ”¹Bag of Words (BoW)\n","\n","Vocabulary of unique words\n","\n","Count of each word\n","\n","Order of words is ignored"],"metadata":{"id":"XIqeMkTGj0-P"}},{"cell_type":"markdown","source":["-------------------------------------------------------------------------"],"metadata":{"id":"j9RXpYBskmka"}},{"cell_type":"markdown","source":["(a) Count Occurrence BoW\n","\n","Each row â†’ document\n","\n","Each column â†’ word\n","\n","Value â†’ number of times word appears"],"metadata":{"id":"H1IXHgU7kLMO"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Create CountVectorizer object\n","count_vectorizer = CountVectorizer()\n","\n","# Fit and transform the documents\n","bow_matrix = count_vectorizer.fit_transform(documents)\n","\n","# Get feature names (vocabulary)\n","feature_names = count_vectorizer.get_feature_names_out()\n","\n","# Convert sparse matrix to array\n","bow_array = bow_matrix.toarray()\n","\n","print(\"Bag of Words - Count Occurrence\")\n","print(feature_names)\n","print(bow_array)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VxxX3382kEnO","executionInfo":{"status":"ok","timestamp":1770090189957,"user_tz":-330,"elapsed":19,"user":{"displayName":"OMKAR SHINDE","userId":"17705111733778157300"}},"outputId":"77fab80c-38c6-4183-9dee-dd6125bf91ea"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Bag of Words - Count Occurrence\n","['and' 'are' 'closely' 'fun' 'is' 'language' 'learning' 'love' 'machine'\n"," 'natural' 'nlp' 'processing' 'related']\n","[[0 0 0 0 0 1 0 1 0 1 0 1 0]\n"," [0 0 0 1 1 1 0 0 0 1 0 1 0]\n"," [0 0 0 0 0 0 1 1 1 0 0 0 0]\n"," [1 1 1 0 0 0 1 0 1 0 1 0 1]]\n"]}]},{"cell_type":"markdown","source":["(b) Normalized Count Occurrence BoW\n","\n","Normalization converts raw counts into relative frequency\n","\n","Useful when documents have different lengths"],"metadata":{"id":"rirz8d3dkQVW"}},{"cell_type":"code","source":["from sklearn.preprocessing import normalize\n","\n","# Create CountVectorizer object (without norm parameter)\n","count_vectorizer_norm = CountVectorizer()\n","\n","# Fit and transform the documents to get the raw counts\n","bow_matrix_raw = count_vectorizer_norm.fit_transform(documents)\n","\n","# Apply L1 normalization to the raw BoW matrix\n","bow_norm_matrix = normalize(bow_matrix_raw, norm='l1', axis=1)\n","\n","print(\"\\nBag of Words - Normalized Count\")\n","print(bow_norm_matrix.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YwOvXP35ku4H","executionInfo":{"status":"ok","timestamp":1770090197575,"user_tz":-330,"elapsed":21,"user":{"displayName":"OMKAR SHINDE","userId":"17705111733778157300"}},"outputId":"e467a502-6962-4e41-9b19-dc3c63c12d07"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Bag of Words - Normalized Count\n","[[0.         0.         0.         0.         0.         0.25\n","  0.         0.25       0.         0.25       0.         0.25\n","  0.        ]\n"," [0.         0.         0.         0.2        0.2        0.2\n","  0.         0.         0.         0.2        0.         0.2\n","  0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.33333333 0.33333333 0.33333333 0.         0.         0.\n","  0.        ]\n"," [0.14285714 0.14285714 0.14285714 0.         0.         0.\n","  0.14285714 0.         0.14285714 0.         0.14285714 0.\n","  0.14285714]]\n"]}]},{"cell_type":"markdown","source":["#ğŸ”¹TF-IDF (Term Frequency â€“ Inverse Document Frequency)\n","\n","TF-IDF reduces the weight of common words\n","and increases the weight of important words"],"metadata":{"id":"MdnRpkgPmWR3"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Create TF-IDF Vectorizer\n","tfidf_vectorizer = TfidfVectorizer()\n","\n","# Fit and transform documents\n","tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n","\n","# Get feature names\n","tfidf_features = tfidf_vectorizer.get_feature_names_out()\n","\n","print(\"\\nTF-IDF Representation\")\n","print(tfidf_features)\n","print(tfidf_matrix.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZUy9RcnsmWDT","executionInfo":{"status":"ok","timestamp":1770090293940,"user_tz":-330,"elapsed":66,"user":{"displayName":"OMKAR SHINDE","userId":"17705111733778157300"}},"outputId":"a44e62e4-70a2-4cad-cae9-d2b7ea6a7d80"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","TF-IDF Representation\n","['and' 'are' 'closely' 'fun' 'is' 'language' 'learning' 'love' 'machine'\n"," 'natural' 'nlp' 'processing' 'related']\n","[[0.         0.         0.         0.         0.         0.5\n","  0.         0.5        0.         0.5        0.         0.5\n","  0.        ]\n"," [0.         0.         0.         0.50867187 0.50867187 0.40104275\n","  0.         0.         0.         0.40104275 0.         0.40104275\n","  0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.57735027 0.57735027 0.57735027 0.         0.         0.\n","  0.        ]\n"," [0.40021825 0.40021825 0.40021825 0.         0.         0.\n","  0.31553666 0.         0.31553666 0.         0.40021825 0.\n","  0.40021825]]\n"]}]},{"cell_type":"markdown","source":["#ğŸ”¹Word Embeddings using Word2Vec\n","\n","Word2Vec converts words into dense vectors\n","\n","Similar words have similar vectors\n","\n","Captures semantic meaning"],"metadata":{"id":"Wqk5HLrgnFpP"}},{"cell_type":"markdown","source":["ğŸ”¹ Implementation using Gensim\n","\n","ğŸ”¹ Step 1: Tokenize Sentences"],"metadata":{"id":"ziACS-OypmeW"}},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","\n","# Tokenize each document into words\n","tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n","\n","print(\"\\nTokenized Documents:\")\n","print(tokenized_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PQ2UctZBnMEW","executionInfo":{"status":"ok","timestamp":1770091202918,"user_tz":-330,"elapsed":449,"user":{"displayName":"OMKAR SHINDE","userId":"17705111733778157300"}},"outputId":"eecc871a-b801-4705-e383-8b5a88f94624"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["\n","Tokenized Documents:\n","[['i', 'love', 'natural', 'language', 'processing'], ['natural', 'language', 'processing', 'is', 'fun'], ['i', 'love', 'machine', 'learning'], ['machine', 'learning', 'and', 'nlp', 'are', 'closely', 'related']]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]}]},{"cell_type":"markdown","source":["ğŸ”¹ Step 2: Train Word2Vec Model"],"metadata":{"id":"zmJYY_guqJ0P"}},{"cell_type":"code","source":["!pip install gensim\n","from gensim.models import Word2Vec\n","\n","# Train Word2Vec model\n","w2v_model = Word2Vec(\n","    sentences=tokenized_docs,\n","    vector_size=50,   # Size of embedding vector\n","    window=3,         # Context window size\n","    min_count=1,      # Minimum word frequency\n","    sg=0              # CBOW model (0 = CBOW, 1 = Skip-gram)\n",")\n","\n","print(\"\\nWord2Vec Vocabulary:\")\n","print(w2v_model.wv.index_to_key)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yg9JzTm_qHXe","executionInfo":{"status":"ok","timestamp":1770091263709,"user_tz":-330,"elapsed":11896,"user":{"displayName":"OMKAR SHINDE","userId":"17705111733778157300"}},"outputId":"8447012b-f1d9-4099-89cb-8bc3c4fbc5e2"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gensim\n","  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n","Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n","Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: gensim\n","Successfully installed gensim-4.4.0\n","\n","Word2Vec Vocabulary:\n","['learning', 'machine', 'processing', 'language', 'natural', 'love', 'i', 'related', 'closely', 'are', 'nlp', 'and', 'fun', 'is']\n"]}]},{"cell_type":"markdown","source":["ğŸ”¹ Step 3: Get Word Embeddings"],"metadata":{"id":"vFHp5HwvqZdX"}},{"cell_type":"code","source":["# Get embedding vector for a word\n","vector_nlp = w2v_model.wv['nlp']\n","\n","print(\"\\nWord2Vec Embedding for 'nlp':\")\n","print(vector_nlp)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ekZs8kvzqZHn","executionInfo":{"status":"ok","timestamp":1770091308819,"user_tz":-330,"elapsed":70,"user":{"displayName":"OMKAR SHINDE","userId":"17705111733778157300"}},"outputId":"648a696d-5fcc-4fe2-df44-fc3fbad8f037"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Word2Vec Embedding for 'nlp':\n","[-1.74570512e-02  4.25888132e-03 -1.74288289e-03 -1.86405703e-02\n"," -1.88638978e-02 -2.82205851e-03  8.86835158e-03  7.40451552e-03\n"," -1.30028781e-02 -1.37532307e-02 -9.99741349e-03 -4.57606837e-03\n"," -1.44995432e-02 -1.92144327e-02 -5.48738195e-03 -1.67285185e-02\n"," -1.20784780e-02 -1.13418382e-02 -4.69252001e-03 -3.41599993e-03\n"," -1.79187208e-02 -1.46756065e-03  1.63107403e-02  1.53871654e-02\n"," -1.44156450e-02 -7.33056432e-03  6.23452151e-03 -1.91431679e-02\n","  2.95561343e-03  1.30518079e-02  1.14972601e-02 -1.75340958e-02\n"," -9.03425831e-03 -1.62877310e-02  9.04490007e-05  1.85266808e-02\n","  1.19484030e-02  1.01412786e-02  1.01219071e-02 -6.48678048e-03\n","  1.91134438e-02 -1.47139747e-02 -1.45456344e-02 -4.53348551e-03\n"," -1.55314908e-03 -6.43321965e-03 -1.18434557e-03  1.49779655e-02\n"," -1.39439374e-03 -3.25323548e-03]\n"]}]},{"cell_type":"markdown","source":["ğŸ”¹ Step 4: Similarity between Words"],"metadata":{"id":"HjyOue7MqfXW"}},{"cell_type":"code","source":["# Find similar words\n","similar_words = w2v_model.wv.most_similar('learning')\n","\n","print(\"\\nWords similar to 'learning':\")\n","print(similar_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QBsPWVapqhk-","executionInfo":{"status":"ok","timestamp":1770091328625,"user_tz":-330,"elapsed":21,"user":{"displayName":"OMKAR SHINDE","userId":"17705111733778157300"}},"outputId":"6446a63b-a833-46e4-fdc6-6f87d2065c90"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Words similar to 'learning':\n","[('and', 0.16716167330741882), ('is', 0.1502092182636261), ('related', 0.132196307182312), ('processing', 0.12670724093914032), ('are', 0.10003447532653809), ('machine', 0.04245268553495407), ('fun', 0.04067608341574669), ('love', 0.012452785857021809), ('nlp', -0.012551533989608288), ('language', -0.014482556842267513)]\n"]}]}]}