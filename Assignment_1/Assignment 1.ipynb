{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNtn09B2vOXp4W2H82dZoul"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#üìò Lab Assignment 1\n","Tokenization, Stemming & Lemmatization using NLTK"],"metadata":{"id":"5myv5E2tjYiQ"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"PndTGECIfY9N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770088342891,"user_tz":-330,"elapsed":8351,"user":{"displayName":"OMKAR SHINDE","userId":"17705111733778157300"}},"outputId":"14c7c7d9-0995-4587-af51-5b6ae6bec5cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"]}],"source":["!pip install nltk"]},{"cell_type":"code","source":["# Import NLTK library\n","import nltk\n","\n","# Download required NLTK datasets\n","# 'punkt' ‚Üí used for tokenization\n","# 'wordnet' ‚Üí used for lemmatization\n","# 'omw-1.4' ‚Üí WordNet related data\n","# 'punkt_tab' ‚Üí specifically for word_tokenize function's internal sentence tokenization\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('punkt_tab')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w8mzdhlWe2tf","executionInfo":{"status":"ok","timestamp":1770088834926,"user_tz":-330,"elapsed":356,"user":{"displayName":"OMKAR SHINDE","userId":"17705111733778157300"}},"outputId":"0289af74-e2ce-4a32-9462-9d52240b2ca2"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Input paragraph containing:\n","# - Multiple punctuation marks\n","# - Contractions\n","# - Emojis\n","# - Hashtags and mentions\n","# - Multi-word expressions\n","\n","text = \"\"\"\n","I enjoyed my winter holiday very much!!! üòä\n","It was nice to have a break from academics, isn't it?\n","I watched many web-series, movies, and YouTube videos...\n","Occasionally, I practiced coding‚ÄîPython, Java, & C++.\n","Learning Natural Language Processing (NLP) is fun!!!\n","Follow me @omkar_shinde #NLP #MachineLearning\n","\"\"\""],"metadata":{"id":"Rae8SVx_e7z_","executionInfo":{"status":"ok","timestamp":1770088351278,"user_tz":-330,"elapsed":13,"user":{"displayName":"OMKAR SHINDE","userId":"17705111733778157300"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["#üîπ PART 1: TOKENIZATION"],"metadata":{"id":"LzlmrdXwhg-2"}},{"cell_type":"markdown","source":["1Ô∏è‚É£ Whitespace Tokenization\n","\n","Splits text only at spaces,\n","Punctuation remains attached to words"],"metadata":{"id":"9Z2GRZAdfL8n"}},{"cell_type":"code","source":["from nltk.tokenize import WhitespaceTokenizer\n","\n","# Create WhitespaceTokenizer object\n","wt = WhitespaceTokenizer()\n","\n","# Tokenize text based only on spaces\n","whitespace_tokens = wt.tokenize(text)\n","\n","print(\"Whitespace Tokenization:\")\n","print(whitespace_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PgXEmBmre92P","executionInfo":{"status":"ok","timestamp":1770088351294,"user_tz":-330,"elapsed":11,"user":{"displayName":"OMKAR SHINDE","userId":"17705111733778157300"}},"outputId":"89435d17-ddd9-4f46-dcb2-5f09d6ed4edd"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Whitespace Tokenization:\n","['I', 'enjoyed', 'my', 'winter', 'holiday', 'very', 'much!!!', 'üòä', 'It', 'was', 'nice', 'to', 'have', 'a', 'break', 'from', 'academics,', \"isn't\", 'it?', 'I', 'watched', 'many', 'web-series,', 'movies,', 'and', 'YouTube', 'videos...', 'Occasionally,', 'I', 'practiced', 'coding‚ÄîPython,', 'Java,', '&', 'C++.', 'Learning', 'Natural', 'Language', 'Processing', '(NLP)', 'is', 'fun!!!', 'Follow', 'me', '@omkar_shinde', '#NLP', '#MachineLearning']\n"]}]},{"cell_type":"markdown","source":["2Ô∏è‚É£ Punctuation-based Tokenization\n","\n","Separates words and punctuation,\n","Useful for fine-grained text analysis"],"metadata":{"id":"koI2yomVfUFm"}},{"cell_type":"code","source":["from nltk.tokenize import wordpunct_tokenize\n","\n","# Split words and punctuation separately\n","punctuation_tokens = wordpunct_tokenize(text)\n","\n","print(\"\\nPunctuation-based Tokenization:\")\n","print(punctuation_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DmiGTcmmfVy2","executionInfo":{"status":"ok","timestamp":1770088398221,"user_tz":-330,"elapsed":26,"user":{"displayName":"OMKAR SHINDE","userId":"17705111733778157300"}},"outputId":"e3ca0c80-b18c-44c6-95d8-aada5ba5c2cc"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Punctuation-based Tokenization:\n","['I', 'enjoyed', 'my', 'winter', 'holiday', 'very', 'much', '!!!', 'üòä', 'It', 'was', 'nice', 'to', 'have', 'a', 'break', 'from', 'academics', ',', 'isn', \"'\", 't', 'it', '?', 'I', 'watched', 'many', 'web', '-', 'series', ',', 'movies', ',', 'and', 'YouTube', 'videos', '...', 'Occasionally', ',', 'I', 'practiced', 'coding', '‚Äî', 'Python', ',', 'Java', ',', '&', 'C', '++.', 'Learning', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'fun', '!!!', 'Follow', 'me', '@', 'omkar_shinde', '#', 'NLP', '#', 'MachineLearning']\n"]}]},{"cell_type":"markdown","source":["3Ô∏è‚É£ Treebank Tokenization\n","\n","Handles contractions properly,   \n","\n","Example: isn't ‚Üí is , n't"],"metadata":{"id":"1wzWOVBzfZKi"}},{"cell_type":"code","source":["from nltk.tokenize import TreebankWordTokenizer\n","\n","# Create Treebank tokenizer\n","tbt = TreebankWordTokenizer()\n","\n","# Tokenize text considering grammar rules\n","treebank_tokens = tbt.tokenize(text)\n","\n","print(\"\\nTreebank Tokenization:\")\n","print(treebank_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cBVV6ZtDfYaW","executionInfo":{"status":"ok","timestamp":1770088466561,"user_tz":-330,"elapsed":38,"user":{"displayName":"OMKAR SHINDE","userId":"17705111733778157300"}},"outputId":"b121277a-dcd6-4326-e87e-0b2a0acdca3b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Treebank Tokenization:\n","['I', 'enjoyed', 'my', 'winter', 'holiday', 'very', 'much', '!', '!', '!', 'üòä', 'It', 'was', 'nice', 'to', 'have', 'a', 'break', 'from', 'academics', ',', 'is', \"n't\", 'it', '?', 'I', 'watched', 'many', 'web-series', ',', 'movies', ',', 'and', 'YouTube', 'videos', '...', 'Occasionally', ',', 'I', 'practiced', 'coding‚ÄîPython', ',', 'Java', ',', '&', 'C++.', 'Learning', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'fun', '!', '!', '!', 'Follow', 'me', '@', 'omkar_shinde', '#', 'NLP', '#', 'MachineLearning']\n"]}]},{"cell_type":"markdown","source":["4Ô∏è‚É£ Tweet Tokenization\n","\n","Handles:\n","\n","Hashtags (#NLP)\n","\n","Mentions (@user)\n","\n","Emojis üòä\n","\n","Best for Twitter / Instagram / social media text"],"metadata":{"id":"Mihe72VNfkZ-"}},{"cell_type":"code","source":["from nltk.tokenize import TweetTokenizer\n","\n","# Create TweetTokenizer object\n","tweet_tokenizer = TweetTokenizer()\n","\n","# Tokenize text suitable for social media\n","tweet_tokens = tweet_tokenizer.tokenize(text)\n","\n","print(\"\\nTweet Tokenization:\")\n","print(tweet_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UR2fXoFkflkp","executionInfo":{"status":"ok","timestamp":1770088610691,"user_tz":-330,"elapsed":47,"user":{"displayName":"OMKAR SHINDE","userId":"17705111733778157300"}},"outputId":"281bc58d-7ffe-4f7e-8814-25c082f73843"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Tweet Tokenization:\n","['I', 'enjoyed', 'my', 'winter', 'holiday', 'very', 'much', '!', '!', '!', 'üòä', 'It', 'was', 'nice', 'to', 'have', 'a', 'break', 'from', 'academics', ',', \"isn't\", 'it', '?', 'I', 'watched', 'many', 'web-series', ',', 'movies', ',', 'and', 'YouTube', 'videos', '...', 'Occasionally', ',', 'I', 'practiced', 'coding', '‚Äî', 'Python', ',', 'Java', ',', '&', 'C', '+', '+', '.', 'Learning', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'fun', '!', '!', '!', 'Follow', 'me', '@omkar_shinde', '#NLP', '#MachineLearning']\n"]}]},{"cell_type":"markdown","source":["5Ô∏è‚É£ MWE (Multi-Word Expression) Tokenization\n","\n","Keeps meaningful phrases together\n","\n","Example: Natural_Language, Machine_Learning"],"metadata":{"id":"YomY-NB_gka2"}},{"cell_type":"code","source":["from nltk.tokenize import MWETokenizer\n","\n","# Define multi-word expressions\n","mwe_tokenizer = MWETokenizer([\n","    ('Natural', 'Language'),\n","    ('Machine', 'Learning')\n","])\n","\n","# Tokenize text while keeping MWEs together\n","mwe_tokens = mwe_tokenizer.tokenize(wordpunct_tokenize(text))\n","\n","print(\"\\nMWE Tokenization:\")\n","print(mwe_tokens)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B6In1TqfgkIp","executionInfo":{"status":"ok","timestamp":1770088760026,"user_tz":-330,"elapsed":10,"user":{"displayName":"OMKAR SHINDE","userId":"17705111733778157300"}},"outputId":"b80977ae-bd97-4035-fbb5-ec02affc02eb"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","MWE Tokenization:\n","['I', 'enjoyed', 'my', 'winter', 'holiday', 'very', 'much', '!!!', 'üòä', 'It', 'was', 'nice', 'to', 'have', 'a', 'break', 'from', 'academics', ',', 'isn', \"'\", 't', 'it', '?', 'I', 'watched', 'many', 'web', '-', 'series', ',', 'movies', ',', 'and', 'YouTube', 'videos', '...', 'Occasionally', ',', 'I', 'practiced', 'coding', '‚Äî', 'Python', ',', 'Java', ',', '&', 'C', '++.', 'Learning', 'Natural_Language', 'Processing', '(', 'NLP', ')', 'is', 'fun', '!!!', 'Follow', 'me', '@', 'omkar_shinde', '#', 'NLP', '#', 'MachineLearning']\n"]}]},{"cell_type":"markdown","source":["#üîπ PART 2: STEMMING"],"metadata":{"id":"l8lz2fOthdN3"}},{"cell_type":"markdown","source":["6Ô∏è‚É£ Porter Stemmer\n","\n","Rule-based stemming\n","\n","May produce non-dictionary words\n","\n","Example: enjoyed ‚Üí enjoy"],"metadata":{"id":"OclgJV5kgxkP"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","\n","# Create Porter Stemmer object\n","ps = PorterStemmer()\n","\n","# Tokenize text into words\n","tokens = word_tokenize(text)\n","\n","# Apply Porter stemming to each word\n","porter_stems = [ps.stem(word) for word in tokens]\n","\n","print(\"\\nPorter Stemming:\")\n","print(porter_stems)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"varnB-5Og5R1","executionInfo":{"status":"ok","timestamp":1770088877276,"user_tz":-330,"elapsed":28,"user":{"displayName":"OMKAR SHINDE","userId":"17705111733778157300"}},"outputId":"c983d008-7317-434f-8e2f-a6ff97930d16"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Porter Stemming:\n","['i', 'enjoy', 'my', 'winter', 'holiday', 'veri', 'much', '!', '!', '!', 'üòä', 'it', 'wa', 'nice', 'to', 'have', 'a', 'break', 'from', 'academ', ',', 'is', \"n't\", 'it', '?', 'i', 'watch', 'mani', 'web-seri', ',', 'movi', ',', 'and', 'youtub', 'video', '...', 'occasion', ',', 'i', 'practic', 'coding‚Äîpython', ',', 'java', ',', '&', 'c++', '.', 'learn', 'natur', 'languag', 'process', '(', 'nlp', ')', 'is', 'fun', '!', '!', '!', 'follow', 'me', '@', 'omkar_shind', '#', 'nlp', '#', 'machinelearn']\n"]}]},{"cell_type":"markdown","source":["7Ô∏è‚É£ Snowball Stemmer\n","\n","Improved version of Porter\n","\n","Supports multiple languages\n","\n","More accurate stemming"],"metadata":{"id":"PwIYRVgwhE8u"}},{"cell_type":"code","source":["from nltk.stem import SnowballStemmer\n","\n","# Create Snowball stemmer for English language\n","sb = SnowballStemmer(\"english\")\n","\n","# Apply Snowball stemming\n","snowball_stems = [sb.stem(word) for word in tokens]\n","\n","print(\"\\nSnowball Stemming:\")\n","print(snowball_stems)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cnc9-HDLhKxN","executionInfo":{"status":"ok","timestamp":1770088872998,"user_tz":-330,"elapsed":35,"user":{"displayName":"OMKAR SHINDE","userId":"17705111733778157300"}},"outputId":"50c826ea-2e25-4bb2-bac6-6e1e8b62cf02"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Snowball Stemming:\n","['i', 'enjoy', 'my', 'winter', 'holiday', 'veri', 'much', '!', '!', '!', 'üòä', 'it', 'was', 'nice', 'to', 'have', 'a', 'break', 'from', 'academ', ',', 'is', \"n't\", 'it', '?', 'i', 'watch', 'mani', 'web-seri', ',', 'movi', ',', 'and', 'youtub', 'video', '...', 'occasion', ',', 'i', 'practic', 'coding‚Äîpython', ',', 'java', ',', '&', 'c++', '.', 'learn', 'natur', 'languag', 'process', '(', 'nlp', ')', 'is', 'fun', '!', '!', '!', 'follow', 'me', '@', 'omkar_shind', '#', 'nlp', '#', 'machinelearn']\n"]}]},{"cell_type":"markdown","source":["#üîπ PART 3: LEMMATIZATION"],"metadata":{"id":"MZrPJQ2chNte"}},{"cell_type":"markdown","source":["8Ô∏è‚É£ WordNet Lemmatization\n","\n","Converts words to dictionary base form\n","\n","Example: studies ‚Üí study\n","\n","More meaningful than stemming"],"metadata":{"id":"PQEE_Md6hSte"}},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","\n","# Create WordNet lemmatizer object\n","lemmatizer = WordNetLemmatizer()\n","\n","# Apply lemmatization to each word\n","lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","print(\"\\nWordNet Lemmatization:\")\n","print(lemmatized_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8LGf4hhKhWrX","executionInfo":{"status":"ok","timestamp":1770088933362,"user_tz":-330,"elapsed":4162,"user":{"displayName":"OMKAR SHINDE","userId":"17705111733778157300"}},"outputId":"a56e5d7f-ec6c-4ec9-f47e-3e44a1ebdee3"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","WordNet Lemmatization:\n","['I', 'enjoyed', 'my', 'winter', 'holiday', 'very', 'much', '!', '!', '!', 'üòä', 'It', 'wa', 'nice', 'to', 'have', 'a', 'break', 'from', 'academic', ',', 'is', \"n't\", 'it', '?', 'I', 'watched', 'many', 'web-series', ',', 'movie', ',', 'and', 'YouTube', 'video', '...', 'Occasionally', ',', 'I', 'practiced', 'coding‚ÄîPython', ',', 'Java', ',', '&', 'C++', '.', 'Learning', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'fun', '!', '!', '!', 'Follow', 'me', '@', 'omkar_shinde', '#', 'NLP', '#', 'MachineLearning']\n"]}]}]}